{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e334368-72cb-4efb-a5b4-f8f0a4de6482",
   "metadata": {},
   "source": [
    "This notebook aims to use pycaret on the CICDDoS2019 dataset split according to the [original release paper](https://ieeexplore.ieee.org/abstract/document/8888419).\n",
    "\n",
    "we show the proportions of Anomaly/Total data points caluclated from inputting the following into the excel sheet\\\n",
    "`=COUNTIF(CK:CK,\"Syn\")/(COUNTIF(CK:CK,\"Syn\")+COUNTIF(CK:CK,\"BENIGN\"))`\n",
    "loading the large csv into the data frame took too much time so this faster method was used.\n",
    "\n",
    "01-12 folder as the training data\n",
    "```\n",
    "['DrDoS_SSDP.csv': 99.971%\n",
    "'DrDoS_NTP.csv': 98.7062%\n",
    "'TFTP.csv', \n",
    "'UDPLag.csv' : 98.9991% of 370166 rows\n",
    "'DrDoS_UDP.csv', 99.8919% of 1048575 rows\n",
    "'Syn.csv' : 99.9966%\n",
    "'DrDoS_MSSQL.csv', \n",
    "'DrDoS_SNMP.csv', \n",
    "'DrDoS_DNS.csv', \n",
    "'DrDoS_LDAP.csv':99.9256%]\n",
    "```\n",
    "\n",
    "03-12 as the testing data\n",
    "```\n",
    "['LDAP.csv':\n",
    " 'MSSQL.csv':\n",
    " 'NetBIOS.csv':\n",
    " 'Portmap.csv': 97.5304% of 191694\n",
    " 'Syn.csv':\n",
    " 'UDP.csv':\n",
    " 'UDPLag.csv': ]\n",
    "```\n",
    "\n",
    "and attempts to use all categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b243ecff-7453-448e-8815-6e3752f04bb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we can assume we have enough BENIGN in each CSV to sample to make the proportion clores to 50%. for example, this is how many BENIGN rows we have in UDPLag.csv:  3704.9914940000053\n"
     ]
    }
   ],
   "source": [
    "print(\"we can assume we have enough BENIGN in each CSV to sample to make the proportion clores to 50%. for example, this is how many BENIGN rows we have in UDPLag.csv: \", 370166* (100-98.9991) / 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da2d10d-c1c9-484b-9f65-017086f751ed",
   "metadata": {},
   "source": [
    "luckally, pycaret has tons of features to address these data issues written below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4079394",
   "metadata": {
    "papermill": {
     "duration": 2.716051,
     "end_time": "2024-04-01T22:12:18.547985",
     "exception": false,
     "start_time": "2024-04-01T22:12:15.831934",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns \n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "import re\n",
    "import pycaret\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4f0f4f-b4e0-4fb4-94bd-738dedda82e0",
   "metadata": {},
   "source": [
    "Function to efficiently read a CSV file into a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3b47f20-cfdc-4ad3-9d92-82d7ab16e13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv_efficiently(file_path):\n",
    "   chunksize = 10000  # Adjust chunksize as needed based on file size and memory\n",
    "   df_chunks = dd.read_csv(file_path, chunksize=chunksize)\n",
    "   df = dd.concat(df_chunks, ignore_index=True)\n",
    "   return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3721d0ff-8090-4db7-86c2-1a012dfe9bec",
   "metadata": {},
   "source": [
    "strictly type and use dask to minimize RAM usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80f9c705-cba5-4266-ba61-d8a74f8ad982",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df, int_cast=False, obj_to_category=True, subset=None):\n",
    "    \"\"\"\n",
    "    Optimizes memory usage of a Dask DataFrame by adjusting dtypes.\n",
    "    \"\"\"\n",
    "    start_mem = df.memory_usage(deep=True).sum().compute() / 1024 ** 2\n",
    "    cols = subset if subset is not None else df.columns\n",
    "\n",
    "    for col in cols:\n",
    "        col_type = df[col].dtype\n",
    "        if col_type != 'object' and col_type != 'string' and not isinstance(col_type, (pd.DatetimeTZDtype, pd.CategoricalDtype, np.dtypes.StrDType)):\n",
    "            try:  # Handle potential typing errors\n",
    "                c_min = df[col].min().compute()\n",
    "                c_max = df[col].max().compute()\n",
    "            except TypeError:\n",
    "                continue  # Skip columns with non-numeric values\n",
    "\n",
    "            # Check for integer conversion\n",
    "            treat_as_int = str(col_type)[:3] == 'int'\n",
    "            if int_cast and not treat_as_int:\n",
    "                treat_as_int = pd.api.types.is_integer_dtype(df[col])\n",
    "\n",
    "            if treat_as_int:\n",
    "                for np_type in [np.int8, np.int16, np.int32, np.int64, np.uint8, np.uint16, np.uint32, np.uint64]:\n",
    "                    if c_min > np.iinfo(np_type).min and c_max < np.iinfo(np_type).max:\n",
    "                        df[col] = df[col].astype(np_type)\n",
    "                        break\n",
    "            else:\n",
    "                for np_type in [np.float16, np.float32, np.float64]:\n",
    "                    # Extract numeric values before comparison\n",
    "                    if c_min > np.finfo(np_type).min and c_max < np.finfo(np_type).max:\n",
    "                        df[col] = df[col].astype(np_type)\n",
    "                        break\n",
    "        #seems to be causing problems. Decreased memory usage by 50.8% with this\n",
    "        #elif not isinstance(col_type, pd.DatetimeTZDtype) and obj_to_category:\n",
    "        #    df[col] = df[col].astype('category')\n",
    "\n",
    "    end_mem = df.memory_usage(deep=True).sum().compute() / 1024 ** 2\n",
    "    print('Memory Usage Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50d4775-efbd-4cd9-b68e-7cd4d3b3da92",
   "metadata": {},
   "source": [
    "listing columns names\\\n",
    "\n",
    "Initially I've tried running pycaret models on 1 CSV at a time with the original features, but they over value source_port as seen [here](https://bec-sv.atlassian.net/wiki/spaces/AN/pages/3538911264/Kentaro+Update+for+4+22+24). So we will remove this for now. we will explore removing each of the 5 tuple flows as the external dataset is from some network that will have different strcuture from whoever uses our ddos model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af47082d-8d8c-4865-a9cd-ef72dc7c8bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['Unnamed: 0', 'Flow ID', ' Source IP', ' Source Port',\n",
    "       ' Destination IP', ' Destination Port', ' Protocol', ' Timestamp',\n",
    "       ' Flow Duration', ' Total Fwd Packets', ' Total Backward Packets',\n",
    "       'Total Length of Fwd Packets', ' Total Length of Bwd Packets',\n",
    "       ' Fwd Packet Length Max', ' Fwd Packet Length Min',\n",
    "       ' Fwd Packet Length Mean', ' Fwd Packet Length Std',\n",
    "       'Bwd Packet Length Max', ' Bwd Packet Length Min',\n",
    "       ' Bwd Packet Length Mean', ' Bwd Packet Length Std', 'Flow Bytes/s',\n",
    "       ' Flow Packets/s', ' Flow IAT Mean', ' Flow IAT Std', ' Flow IAT Max',\n",
    "       ' Flow IAT Min', 'Fwd IAT Total', ' Fwd IAT Mean', ' Fwd IAT Std',\n",
    "       ' Fwd IAT Max', ' Fwd IAT Min', 'Bwd IAT Total', ' Bwd IAT Mean',\n",
    "       ' Bwd IAT Std', ' Bwd IAT Max', ' Bwd IAT Min', 'Fwd PSH Flags',\n",
    "       ' Bwd PSH Flags', ' Fwd URG Flags', ' Bwd URG Flags',\n",
    "       ' Fwd Header Length', ' Bwd Header Length', 'Fwd Packets/s',\n",
    "       ' Bwd Packets/s', ' Min Packet Length', ' Max Packet Length',\n",
    "       ' Packet Length Mean', ' Packet Length Std', ' Packet Length Variance',\n",
    "       'FIN Flag Count', ' SYN Flag Count', ' RST Flag Count',\n",
    "       ' PSH Flag Count', ' ACK Flag Count', ' URG Flag Count',\n",
    "       ' CWE Flag Count', ' ECE Flag Count', ' Down/Up Ratio',\n",
    "       ' Average Packet Size', ' Avg Fwd Segment Size',\n",
    "       ' Avg Bwd Segment Size', ' Fwd Header Length.1', 'Fwd Avg Bytes/Bulk',\n",
    "       ' Fwd Avg Packets/Bulk', ' Fwd Avg Bulk Rate', ' Bwd Avg Bytes/Bulk',\n",
    "       ' Bwd Avg Packets/Bulk', 'Bwd Avg Bulk Rate', 'Subflow Fwd Packets',\n",
    "       ' Subflow Fwd Bytes', ' Subflow Bwd Packets', ' Subflow Bwd Bytes',\n",
    "       'Init_Win_bytes_forward', ' Init_Win_bytes_backward',\n",
    "       ' act_data_pkt_fwd', ' min_seg_size_forward', 'Active Mean',\n",
    "       ' Active Std', ' Active Max', ' Active Min', 'Idle Mean', ' Idle Std',\n",
    "       ' Idle Max', ' Idle Min', 'SimillarHTTP', ' Inbound', ' Label']\n",
    "ignore_columns = ['Flow Bytes/s', ' Flow Packets/s', ' Source IP', ' Source Port', ' Destination IP', ' Destination Port',]\n",
    "use_columns = [x for x in columns if x not in ignore_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "388808ed-1ae4-404f-89e6-66d217acdbb5",
   "metadata": {},
   "source": [
    "function to combine the df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d12316f1-a1db-4167-b31c-fdf2af11ebad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_df(csv_dir, categories):\n",
    "    df = dd.from_pandas(pd.DataFrame(columns=use_columns), npartitions=1)\n",
    "    for ddos_type in categories:\n",
    "        file_path = os.path.join(csv_dir, ddos_type)\n",
    "        \n",
    "        # pre-defining these removes some bugs\n",
    "        dtype={'SimillarHTTP': 'object', ' Label': 'object', 'Flow_ID' : 'object', ' Source_IP' : 'object', ' Destination IP': 'object', ' TimeStamp': 'object'}\n",
    "        df = dd.concat([df, reduce_mem_usage(dd.read_csv(file_path, dtype=dtype))], ignore_index=True)\n",
    "        \n",
    "    # Check for potential issues and handle them as needed\n",
    "    if df.isnull().values.any():\n",
    "       print(\"Warning: DataFrame contains missing values. Consider handling them.\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a936135d-f3fc-42d5-89b3-8af1c50fca1b",
   "metadata": {},
   "source": [
    "Read all of the CSVs into one Data Frame. RegEx to remove special characters like '/' that cause errors in pycaret and make column name more readable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa67f24f-4f4a-451c-963e-b19b789b3514",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory Usage Decreased by 55.3%\n",
      "Memory Usage Decreased by 53.2%\n",
      "Warning: DataFrame contains missing values. Consider handling them.\n"
     ]
    }
   ],
   "source": [
    "# CSV file directory and file names for training\n",
    "train_dir ='C:\\\\Users\\\\ktv07101\\\\Desktop\\\\BHNI Anomaly Detection Related\\\\CondaModelReplication\\\\CICDDoS2019\\\\CSV-01-12\\\\01-12'\n",
    "train_ddos_categories = ['Syn.csv', 'DrDoS_LDAP.csv']\n",
    "train_df = combine_df(train_dir, train_ddos_categories).rename(columns=lambda x: x.replace('/', '_').replace(' ', '_')).rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e355d4e-5ba7-49e9-ad83-6ba3b4d33c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV file directory and file names for testing\n",
    "test_dir = 'C:\\\\Users\\\\ktv07101\\\\Desktop\\\\BHNI Anomaly Detection Related\\\\CondaModelReplication\\\\CICDDoS2019\\\\CSV-03-11\\\\03-11'\n",
    "test_ddos_categories = ['LDAP.csv', 'Syn.csv']\n",
    "test_df = combine_df(test_dir, test_ddos_categories).rename(columns=lambda x: x.replace('/', '_').replace(' ', '_')).rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6acfc570-a58c-4a66-bbd8-1792fb0dba4a",
   "metadata": {},
   "source": [
    "Setting up [pycaret](https://pycaret.readthedocs.io/en/latest/api/classification.html#pycaret.classification.setup) to test 5 models.\\\n",
    "we are using the following features for the following reasons:\\\n",
    "`fix_imbalance` balances the distributions\\\n",
    "`transformation` makes the distribution gaussian\\\n",
    "`normalize` normalizes features\\\n",
    "`pca` reduces the features we use\\\n",
    "`pca_method` is `incremental` for we are using a large dataset\\\n",
    "`feature_selection` selects few features to use\\\n",
    "`n_features_to_select` specifies how many or fractions of features to start with\\\n",
    "`numeric_imputation` is done via knn to accurately impute\\\n",
    "`imputation_type` is `iterative` to be accurate\\\n",
    "`polynomial_features` generates features. can adjust n variables allowed with\\\n",
    "`log_experiment` to view the experiments\\\n",
    "`use_gpu` to speed up algorithms\\\n",
    "\n",
    "If we want to use algorithms that can't handle categories, set \"max_encoding_ohe=-1\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e200fde",
   "metadata": {
    "papermill": {
     "duration": 745.162199,
     "end_time": "2024-04-02T01:46:06.489380",
     "exception": false,
     "start_time": "2024-04-02T01:33:41.327181",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pycaret.classification import *\n",
    "s = setup(test_data=test_df, log_plots=True, log_profile=True, load_data=True, profile=True, use_gpu=True, data=train_df, target='_Label', log_experiment=True, feature_selection=True, n_features_to_select=0.5, numeric_imputation='knn',  imputation_type='iterative', polynomial_features=True, remove_multicollinearity=True, fix_imbalance=True, normalize=True, transformation=True, pca=True, pca_method='incremental')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d79773a",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "best = compare_models(budget_time=420)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f836c7e7",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "preds = predict_model(best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852c0257",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_model(best, plot='confusion_matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30788a1",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_model(best, plot='auc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f316f6eb",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_model(best, plot='class_report')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0002bd",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_model(best, plot='feature')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb0ca78f",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "result = predict_model(best, data=combined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c10f91",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6d089e",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "result.query('Type == prediction_label').shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4012de79",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be49f2be",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "result = result[['TimeDateStamp', 'AddressOfEntryPoint', 'SizeOfInitializedData', 'SizeOfCode', 'SizeOfImage', 'Type', 'prediction_label', 'prediction_score']]"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 4658944,
     "sourceId": 7927234,
     "sourceType": "datasetVersion"
    }
   ],
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 12835.242828,
   "end_time": "2024-04-02T01:46:06.589489",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-04-01T22:12:11.346661",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
